{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "290d672f",
   "metadata": {},
   "source": [
    "IMPORTANT NOTE: This notebook is for learning and entertainment purposes only. Please consult robot.txt files before scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d56dd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Note: This class needs some serious rewriting before general usage. \n",
    "# For the purposes of creating this corpus, it'll do...\n",
    "class web_crawl():\n",
    "\tdef __init__(self, allowed_domains: list[str]):\n",
    "\t\t\n",
    "\t\t# Set up chrome driver with options\n",
    "\t\toptions = Options()\n",
    "\t\toptions.add_argument(\"--headless\")\n",
    "\t\toptions.add_argument(\"--disable-gpu\")\n",
    "\t\tdriver = webdriver.Chrome(service=Service(ChromeDriverManager().install()),\n",
    "\t\t\t\t\t\t\t\toptions=options)\n",
    "\t\t\n",
    "\t\t# Initialize variables\n",
    "\t\tself.driver = driver\n",
    "\t\tself.links = []\n",
    "\t\tself.directories = []\n",
    "\t\tself.result = []\n",
    "\t\tself.allowed_domains = allowed_domains\n",
    "\t\n",
    "\tdef get_soup(self, link: str):\n",
    "\n",
    "\t\t# Navigate to URL\n",
    "\t\tself.driver.get(link)\n",
    "\n",
    "\t\t# Retrieve the page source\n",
    "\t\thtml = self.driver.page_source\n",
    "\n",
    "\t\t# Parse the HTML with BeautifulSoup\n",
    "\t\treturn BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "\tdef web_scrape(self, link: str): # User: Make sure link is in list of allowed domains\n",
    "\t\t\n",
    "\t\t# initialize variables\n",
    "\t\ttext = ''\n",
    "\t\tlinks = []\n",
    "\t\tsoup = self.get_soup(link)\n",
    "\t\t\n",
    "\t\t# let's assume all the text is wrapped around <p> tags.\n",
    "\t\tfor element in soup.find_all('p'):\n",
    "\t\t\ttext += (element.text) + ' '\n",
    "\t\t\n",
    "\t\t# let's assume some of the text is wrapped around <span> tags.\n",
    "\t\tfor element in soup.find_all('span'):\n",
    "\t\t\ttext += (element.text) + ' '\n",
    "\n",
    "\t\t# let's assume some of the text is wrapped around <a> tags.\n",
    "\t\tfor element in soup.find_all('a'):\n",
    "\t\t\ttext += (element.text) + ' '\n",
    "\n",
    "\t\t# let's assume all the links are in blocks with href.\n",
    "\t\tfor element in soup.find_all('a', href=True):\n",
    "\t\t\thref = element['href']\n",
    "\t\t\tif (self.check_domain(href)) and (href not in self.links):\n",
    "\t\t\t\tlinks.append(href)\n",
    "\t\t\tif (href.startswith('/')) and (href not in self.directories):\n",
    "\t\t\t\tlinks.append(link + href)\n",
    "\t\t\t\tself.directories.append(href)\n",
    "\n",
    "\t\t# Output ordered pair\n",
    "\t\treturn text, links\n",
    "\t\t\n",
    "\tdef dfs(self, link: str): # User: Make sure link is in list of allowed domains\n",
    "\t\t\n",
    "\t\t# Web scrape\n",
    "\t\ttext, links = self.web_scrape(link=link)\n",
    "\t\tif (link, text) not in self.result: # Check unique strings\n",
    "\t\t\tself.result.append({link : text})\n",
    "\t\t\tprint(f\"Scraped {link}\")\n",
    "\t\t\n",
    "\t\t# Mark link as explored\n",
    "\t\tif link not in self.links:\n",
    "\t\t\tself.links.append(link)\n",
    "\t\t\n",
    "\t\t# Visit neighbors\n",
    "\t\tfor neighbor in links:\n",
    "\t\t\tif neighbor not in self.links:\n",
    "\t\t\t\tself.dfs(neighbor)\n",
    "\n",
    "\tdef check_link(self, link: str):\n",
    "\t\t\n",
    "\t\t# Check for unallowed links\n",
    "\t\tunallowed_links = ['https://.com', 'https://.org']\n",
    "\t\tif (link in unallowed_links):\n",
    "\t\t\treturn False \n",
    "\t\t\n",
    "\t\t# Check for mailing links\n",
    "\t\tif (\"mailto:\" in link):\n",
    "\t\t\treturn False\n",
    "\t\t\n",
    "\t\t# Check if it's a link\n",
    "\t\treturn (\"https://\" in link and (\".com\" in link or \".org\" in link))\n",
    "\n",
    "\tdef check_domain(self, link: str):\n",
    "\t\t\n",
    "\t\t# Check if it's even a link\n",
    "\t\tif (not self.check_link(link)):\n",
    "\t\t\treturn False\n",
    "\t\t\n",
    "\t\t# Get domain\n",
    "\t\ttoken = link.split('https://')[1].split('/')[0]\n",
    "\t\tdomain = (token.split('.')[-2]+'.' + token.split('.')[-1]).split('.')[0]\n",
    "\t\t\n",
    "\t\t# Check for allowed domains\n",
    "\t\treturn (domain in self.allowed_domains)\n",
    "\n",
    "\t\n",
    "\tdef __del__(self):\n",
    "\t\t\n",
    "\t\t# Close the driver\n",
    "\t\tself.driver.quit()\n",
    "\t\t"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
