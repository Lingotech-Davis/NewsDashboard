{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8d4a772",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\AarPi\\OneDrive\\Desktop\\NewsDashboard\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from newspaper import Article\n",
    "from newspaper.article import ArticleException\n",
    "from keybert import KeyBERT\n",
    "from scipy.spatial import distance\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from api_test import ask_gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fc26a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "NEWS_API_KEY = os.getenv(\"NEWS_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0087b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "# ONLY RUN WHEN NEEDED\n",
    "url = ('https://newsapi.org/v2/top-headlines?'\n",
    "       'q=Trump&'\n",
    "       'from=2025-07-00&'\n",
    "       'sortBy=relevancy&'\n",
    "       'pageSize=100&'\n",
    "       f'apiKey={NEWS_API_KEY}')\n",
    "\n",
    "response = requests.get(url)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e25b702",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_news_api(keyword, date, everything=True):\n",
    "    API_KEY = NEWS_API_KEY\n",
    "    \n",
    "    # choose endpoint\n",
    "    endpoint = \"everything\" if everything else \"top-headlines\"\n",
    "    # build URL with the passed-in variables\n",
    "    url = (\n",
    "        f\"https://newsapi.org/v2/{endpoint}\"\n",
    "        f\"?q={keyword}\"\n",
    "        f\"&from={date}\"\n",
    "        \"&sortBy=relevancy\"\n",
    "        f\"&apiKey={API_KEY}\"\n",
    "    )\n",
    "    response = requests.get(url)\n",
    "\n",
    "    return response\n",
    "    \n",
    "def search_article_type(lean_list, all_articles, ratio):\n",
    "    \"\"\"\n",
    "    combs through articles and returns N amount of a certain type of article, meaning N amount of left, right, and center\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    i = 0\n",
    "    type_list = []\n",
    "    while count < ratio:\n",
    "        i += 1\n",
    "        if(all_articles[i]['source']['name'] in lean_list):\n",
    "            type_list.append(all_articles[i])\n",
    "            count += 1\n",
    "\n",
    "    return type_list\n",
    "\n",
    "def get_blend(all_articles, n_search):\n",
    "    blended_articles = []\n",
    "    \n",
    "    # compile list of sources for each type\n",
    "    right_leaning = ['Breitbart News', ]\n",
    "    left_leaning = []\n",
    "    center_leaning = ['NBC News','The Washington Post','ABC News',]\n",
    "\n",
    "    # how much of each type of source to get. this automatically assumes an equal blend of each source\n",
    "    ratio = n_search // 3       \n",
    "    \n",
    "    # loop through each type\n",
    "    types = ['right', 'left', 'center']\n",
    "    for x in types:\n",
    "        result = search_article_type(x, all_articles, ratio)\n",
    "        blended_articles.append(result)\n",
    "\n",
    "    return blended_articles\n",
    "\n",
    "def rewrite_content(article_url):\n",
    "    \"\"\"\n",
    "    given a url to some article, return its content\n",
    "    this is really just a helper function. also has safeguards for articles that block the scraping tool. \n",
    "    \"\"\"\n",
    "    article = Article(article_url)\n",
    "    try: \n",
    "        article.download()\n",
    "        article.parse()\n",
    "    \n",
    "        # cleaning it up\n",
    "        text = article.text\n",
    "        filtered_lines = filter(str.strip, text.splitlines())\n",
    "        cleaned_text = \"\\n\".join(filtered_lines)\n",
    "        return cleaned_text\n",
    "\n",
    "    except ArticleException as e:\n",
    "        print(f\"[!] Skipping article at {url} — {e}\")\n",
    "        return None\n",
    "    \n",
    "def get_keyword(query):\n",
    "    \"\"\"\n",
    "    simply return the main keyword of the query the user has asked\n",
    "    \"\"\"\n",
    "    keyword_model = KeyBERT('distilbert-base-nli-mean-tokens')\n",
    "    keywords = keyword_model.extract_keywords(query)            # a list of keywords with the most relevant listed first\n",
    "\n",
    "    return keywords[0][0]\n",
    "\n",
    "def get_similarities(all_articles, query):\n",
    "    \"\"\"\n",
    "    sort the articles in order of their semantic similarity to the query \n",
    "    return only the top n results\n",
    "    need to add a \"score\" attribute to all of the articles\n",
    "    \"\"\"\n",
    "    semantic_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    encoded_query = semantic_model.encode(query)\n",
    "\n",
    "    for article in all_articles:\n",
    "        encoded_description = semantic_model.encode(article['description'])\n",
    "        article[\"sem_sim\"] = 1-distance.cosine(encoded_query, encoded_description)\n",
    "\n",
    "def get_n_matches(all_articles, N):\n",
    "    \"\"\"\n",
    "    all_articles is a list of dictionaries. need to sort this list to get N of the dictionaries with the largest sem_sim value\n",
    "    \"\"\"\n",
    "    sorted_articles = sorted(all_articles, key=lambda x: x['sem_sim'], reverse=True)\n",
    "    \n",
    "    return sorted_articles[:N]\n",
    "    \n",
    "def extract_content(article_list):\n",
    "    \"\"\"\n",
    "    given a list of articles, get the content for them\n",
    "    save this in a text file\n",
    "    \"\"\"\n",
    "    all_content = \"\"\n",
    "    article_list\n",
    "\n",
    "    for article in article_list:\n",
    "        content = rewrite_content(article['url'])\n",
    "\n",
    "        # skip articles that contain nothing or that block the scraper\n",
    "        if (article['content'] == None) or (content == None):\n",
    "            continue\n",
    "        article['content'] = content\n",
    "        all_content += article['source']['name'] + \"\\n\"\n",
    "        all_content += article['content'] + \"\\n----------------------------\\n\"\n",
    "\n",
    "    return all_content\n",
    "\n",
    "def get_citation(articles):\n",
    "    citation = \"Response is based on the following sources: \\n\"\n",
    "\n",
    "    for article in articles:\n",
    "        citation += (\n",
    "                        f\"{article['author']}. \"\n",
    "                        f\"\\\"{article['title']}\\\". \"\n",
    "                        f\"{article['source']['name']}. \"\n",
    "                        f\"{article['url']}\\n\"\n",
    "                    )\n",
    "    \n",
    "    return citation\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb4e6ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] Skipping article at https://newsapi.org/v2/top-headlines?q=Trump&from=2025-07-00&sortBy=relevancy&pageSize=100&apiKey=05c5488a934a4e33a702e25364b898c1 — Article `download()` failed with 403 Client Error: Forbidden for url: https://thespun.com/soccer/everyone-had-same-reaction-to-photo-of-president-trumps-ankles on URL https://thespun.com/soccer/everyone-had-same-reaction-to-photo-of-president-trumps-ankles\n",
      "Yes, based solely on the articles provided, it is stated that President Donald Trump's \"signature 'One Big Beautiful Bill' ... will offer tax breaks to the wealthy while kicking millions off of Medicaid.\"\n",
      "\n",
      "One article also notes that Elon Musk \"has issued no objection to the cuts to food benefits and healthcare,\" implying that cuts to healthcare are part of this bill.\n",
      "Response is based on the following sources: \n",
      "AJ Dellinger. \"Elon Musk Rekindles Trump Criticism, Attacks ‘Big, Beautiful Bill’\". Gizmodo.com. https://gizmodo.com/elon-musk-rekindles-trump-criticism-attacks-big-beautiful-bill-2000624326\n",
      "Richard Lawler. \"Trump says he’ll look into deporting Elon as fight over bill escalates\". The Verge. https://www.theverge.com/news/695703/trump-elon-musk-deport-threat-doge-subsidies-spending-bill\n",
      "Tzvi Machlin. \"Everyone Had Same Reaction To Photo Of President Trump's Ankles\". The Spun. https://thespun.com/soccer/everyone-had-same-reaction-to-photo-of-president-trumps-ankles\n",
      "Brittney Melton. \"SCOTUS allows dismantling of Education Dept. And, Trump threatens Russia with tariffs\". NPR. https://www.npr.org/2025/07/15/g-s1-77615/up-first-newsletter-moscow-trump-education-department-epstein-files\n",
      "Luc Olinga. \"Trump Tells Musk to “Head Back Home to South Africa” in Escalating Feud\". Gizmodo.com. https://gizmodo.com/trump-tells-musk-to-head-back-home-to-south-africa-in-escalating-feud-2000622639\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# MAIN\n",
    "def main():\n",
    "    # user input\n",
    "    query = \"Trump wants to defund healthcare\"\n",
    "    date = \"2025-07-01\"\n",
    "    everything = True\n",
    "    n_search = 5             # how many articles to base the llm response on\n",
    "    blend = False             # TODO: get an even mix of right, center, and left wing sources\n",
    "    \n",
    "    # put this keyword into the API search as its \"q\" value and sort by relevance\n",
    "    keyword = get_keyword(query)\n",
    "\n",
    "    # call the news_api with a set of parameters, remember there's a limit so comment this out whenever you can.\n",
    "    # response = call_news_api(keyword, date, everything)\n",
    "    response_data = response.json()\n",
    "\n",
    "    # format the articles into a list called all_articles. each item in this list is a dictionary\n",
    "    all_articles = response_data.get('articles', [])   \n",
    "    \n",
    "    # adds a semantic similarity score between the query and the article's description to the dictionary\n",
    "    get_similarities(all_articles, query)\n",
    "\n",
    "    # gets the N articles with the best semantic similarity scores\n",
    "    top_matches = get_n_matches(all_articles, n_search)\n",
    "\n",
    "    if blend:\n",
    "        top_matches = get_blend(all_articles, n_search)\n",
    "    \n",
    "    # rewrite the content of each article AND also return a string that's all of the articles content\n",
    "    top_content = extract_content(top_matches)\n",
    "    \n",
    "    # save this to a txt file for our own viewing\n",
    "    output_file = open(\"top_content.txt\", \"w\", encoding=\"utf-8\")\n",
    "    output_file.write(top_content)    \n",
    "    output_file.close()\n",
    "\n",
    "    # ask gemini to search the contents based on the query\n",
    "    prompt = \"Based soley on the articles provided, is it true that \"\n",
    "    prompt += query\n",
    "    gemini_response = ask_gemini(prompt, top_content, test_mode=False)    # set test mode to true to just call the function without the API. formatting purposes, avoiding too many API requests\n",
    "    print(gemini_response)\n",
    "\n",
    "    # return the response and the list of sources used\n",
    "    citation = get_citation(top_matches)\n",
    "    print(citation)\n",
    "    \n",
    "    # TODO: finish the list of sources for right, center, and left leaning sources. Limit is 5min a day. \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cee7976",
   "metadata": {},
   "source": [
    "https://newspaper.readthedocs.io/en/latest/\n",
    "- easy webscraping of news sources https://scrapeops.io/python-web-scraping-playbook/newspaper3k/\n",
    "\n",
    "\n",
    "News API gets you the most relevant articles, authors, description etc. \n",
    "BUT it cannot get you the actual content of each article but newspaper3k can. it can also give us the main keywords, although it's not reliable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13511acc",
   "metadata": {},
   "source": [
    "now that we have all the stories for a certain keyword in one string we have several options\n",
    " - use it as ground truth fact checker\n",
    "   - remember that this is based on a time range from when we set our API request to and what keyword\n",
    " - provide a summary of it all\n",
    "   - convinenet because we also a have a list of sources used\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977b4815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SUMMARIZATION\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "model_name = 't5-small'\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# basic summarization function\n",
    "def summarize(text):\n",
    "    inputs = tokenizer.encode(\"summarize: \" + text, return_tensors=\"pt\", max_length=5120, truncation=True)\n",
    "    summary_ids = model.generate(inputs, max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    return summary\n",
    "\n",
    "# basic summarizer, note its not very good.\n",
    "# misses a lot of key details and ignores many stories, even after increasing the max length 5 fold. \n",
    "# probably needs some fine tuning\n",
    "text = all_articles\n",
    "summary = summarize(text)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3aee94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine tune summarization https://huggingface.co/docs/transformers/en/tasks/summarization\n",
    "# use the provided descriptions as targets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
