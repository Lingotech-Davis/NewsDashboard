{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8d4a772",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from newspaper import Article\n",
    "from newspaper.article import ArticleException\n",
    "from keybert import KeyBERT\n",
    "from scipy.spatial import distance\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from api_test import ask_gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0087b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [401]>\n"
     ]
    }
   ],
   "source": [
    "# ONLY RUN WHEN NEEDED\n",
    "url = ('https://newsapi.org/v2/top-headlines?'\n",
    "       'q=Trump&'\n",
    "       'from=2025-07-00&'\n",
    "       'sortBy=relevancy&'\n",
    "       'pageSize=100&'\n",
    "       'apiKey=')\n",
    "\n",
    "response = requests.get(url)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e25b702",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_news_api(keyword, date, everything=True):\n",
    "    API_KEY = \"\"\n",
    "    \n",
    "    # choose endpoint\n",
    "    endpoint = \"everything\" if everything else \"top-headlines\"\n",
    "    # build URL with the passed-in variables\n",
    "    url = (\n",
    "        f\"https://newsapi.org/v2/{endpoint}\"\n",
    "        f\"?q={keyword}\"\n",
    "        f\"&from={date}\"\n",
    "        \"&sortBy=relevancy\"\n",
    "        f\"&apiKey={API_KEY}\"\n",
    "    )\n",
    "    response = requests.get(url)\n",
    "\n",
    "    return response\n",
    "    \n",
    "def search_article_type(lean_list, all_articles, ratio):\n",
    "    \"\"\"\n",
    "    combs through articles and returns N amount of a certain type of article, meaning N amount of left, right, and center\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    i = 0\n",
    "    type_list = []\n",
    "    while count < ratio:\n",
    "        i += 1\n",
    "        if(all_articles[i]['source']['name'] in lean_list):\n",
    "            type_list.append(all_articles[i])\n",
    "            count += 1\n",
    "\n",
    "    return type_list\n",
    "\n",
    "def get_blend(all_articles, n_search):\n",
    "    blended_articles = []\n",
    "    \n",
    "    # compile list of sources for each type\n",
    "    right_leaning = ['Breitbart News', ]\n",
    "    left_leaning = []\n",
    "    center_leaning = ['NBC News','The Washington Post','ABC News',]\n",
    "\n",
    "    # how much of each type of source to get. this automatically assumes an equal blend of each source\n",
    "    ratio = n_search // 3       \n",
    "    \n",
    "    # loop through each type\n",
    "    types = ['right', 'left', 'center']\n",
    "    for x in types:\n",
    "        result = search_article_type(x, all_articles, ratio)\n",
    "        blended_articles.append(result)\n",
    "\n",
    "    return blended_articles\n",
    "\n",
    "def rewrite_content(article_url):\n",
    "    \"\"\"\n",
    "    given a url to some article, return its content\n",
    "    this is really just a helper function. also has safeguards for articles that block the scraping tool. \n",
    "    \"\"\"\n",
    "    article = Article(article_url)\n",
    "    try: \n",
    "        article.download()\n",
    "        article.parse()\n",
    "    \n",
    "        # cleaning it up\n",
    "        text = article.text\n",
    "        filtered_lines = filter(str.strip, text.splitlines())\n",
    "        cleaned_text = \"\\n\".join(filtered_lines)\n",
    "        return cleaned_text\n",
    "\n",
    "    except ArticleException as e:\n",
    "        print(f\"[!] Skipping article at {url} — {e}\")\n",
    "        return None\n",
    "    \n",
    "def get_keyword(query):\n",
    "    \"\"\"\n",
    "    simply return the main keyword of the query the user has asked\n",
    "    \"\"\"\n",
    "    keyword_model = KeyBERT('distilbert-base-nli-mean-tokens')\n",
    "    keywords = keyword_model.extract_keywords(query)            # a list of keywords with the most relevant listed first\n",
    "\n",
    "    return keywords[0][0]\n",
    "\n",
    "def get_similarities(all_articles, query):\n",
    "    \"\"\"\n",
    "    sort the articles in order of their semantic similarity to the query \n",
    "    return only the top n results\n",
    "    need to add a \"score\" attribute to all of the articles\n",
    "    \"\"\"\n",
    "    semantic_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    encoded_query = semantic_model.encode(query)\n",
    "\n",
    "    for article in all_articles:\n",
    "        encoded_description = semantic_model.encode(article['description'])\n",
    "        article[\"sem_sim\"] = 1-distance.cosine(encoded_query, encoded_description)\n",
    "\n",
    "def get_n_matches(all_articles, N):\n",
    "    \"\"\"\n",
    "    all_articles is a list of dictionaries. need to sort this list to get N of the dictionaries with the largest sem_sim value\n",
    "    \"\"\"\n",
    "    sorted_articles = sorted(all_articles, key=lambda x: x['sem_sim'], reverse=True)\n",
    "    \n",
    "    return sorted_articles[:N]\n",
    "    \n",
    "def extract_content(article_list):\n",
    "    \"\"\"\n",
    "    given a list of articles, get the content for them\n",
    "    save this in a text file\n",
    "    \"\"\"\n",
    "    all_content = \"\"\n",
    "    article_list\n",
    "\n",
    "    for article in article_list:\n",
    "        content = rewrite_content(article['url'])\n",
    "\n",
    "        # skip articles that contain nothing or that block the scraper\n",
    "        if (article['content'] == None) or (content == None):\n",
    "            continue\n",
    "        article['content'] = content\n",
    "        all_content += article['source']['name'] + \"\\n\"\n",
    "        all_content += article['content'] + \"\\n----------------------------\\n\"\n",
    "\n",
    "    return all_content\n",
    "\n",
    "def get_citation(articles):\n",
    "    citation = \"Response is based on the following sources: \\n\"\n",
    "\n",
    "    for article in articles:\n",
    "        citation += (\n",
    "                        f\"{article['author']}. \"\n",
    "                        f\"\\\"{article['title']}\\\". \"\n",
    "                        f\"{article['source']['name']}. \"\n",
    "                        f\"{article['url']}\\n\"\n",
    "                    )\n",
    "    \n",
    "    return citation\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb4e6ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] Skipping article at https://newsapi.org/v2/top-headlines?q=Trump&from=2025-07-00&sortBy=relevancy&pageSize=100&apiKey=05c5488a934a4e33a702e25364b898c1 — Article `download()` failed with 429 Client Error: Unknown Error for url: https://thehill.com/homenews/administration/5390668-trump-administration-epstein-case/ on URL https://thehill.com/homenews/administration/5390668-trump-administration-epstein-case/\n",
      "[!] Skipping article at https://newsapi.org/v2/top-headlines?q=Trump&from=2025-07-00&sortBy=relevancy&pageSize=100&apiKey=05c5488a934a4e33a702e25364b898c1 — Article `download()` failed with 403 Client Error: Forbidden 1002 for url: https://www.foxnews.com/politics/supreme-court-lets-trumps-wrecking-ball-federal-job-cuts-proceed-legal-fight-continues on URL https://www.foxnews.com/politics/supreme-court-lets-trumps-wrecking-ball-federal-job-cuts-proceed-legal-fight-continues\n",
      "Based solely on the articles provided, there is no information about whether Trump wants to defund healthcare. The articles focus on President Trump's redecoration of the White House, his taste in art and furnishings, and details from a Cabinet meeting.\n",
      "Response is based on the following sources: \n",
      "Rebecca Beitsch, Emily Brooks, Brett Samuels. \"Trump administration takes blows from Epstein conspiracy community it once embraced\". The Hill. https://thehill.com/homenews/administration/5390668-trump-administration-epstein-case/\n",
      "Jasmine Baehr, Shannon Bream, Bill Mears. \"Supreme Court lets Trump’s ‘wrecking ball’ federal job cuts proceed while legal fight continues\". Fox News. https://www.foxnews.com/politics/supreme-court-lets-trumps-wrecking-ball-federal-job-cuts-proceed-legal-fight-continues\n",
      "Swapna Venugopal Ramaswamy. \"Trump shows off his handpicked Cabinet Room decorations\". USA Today. https://www.usatoday.com/story/news/politics/2025/07/08/trump-redecorates-white-house-cabinet-room/84511386007/\n",
      "Sybilla Gross. \"Gold (XAUUSD) Holds Decline as Extended US Negotiations Ease Trade Fears\". Bloomberg. https://www.bloomberg.com/news/articles/2025-07-08/gold-xauusd-holds-decline-as-extended-us-negotiations-ease-trade-fears\n",
      "Matt Viser. \"In Cabinet meeting, Trump takes a golden opportunity to talk interior decor\". The Washington Post. https://www.washingtonpost.com/politics/2025/07/08/cabinet-meeting-trump-takes-golden-opportunity-talk-interior-decor/\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# MAIN\n",
    "def main():\n",
    "    # user input\n",
    "    query = \"Trump wants to defund healthcare\"\n",
    "    date = \"2025-07-01\"\n",
    "    everything = True\n",
    "    n_search = 5             # how many articles to base the llm response on\n",
    "    blend = False             # TODO: get an even mix of right, center, and left wing sources\n",
    "    \n",
    "    # put this keyword into the API search as its \"q\" value and sort by relevance\n",
    "    keyword = get_keyword(query)\n",
    "\n",
    "    # call the news_api with a set of parameters, remember there's a limit so comment this out whenever you can.\n",
    "    response = call_news_api(keyword, date, everything)\n",
    "    response_data = response.json()\n",
    "\n",
    "    # format the articles into a list called all_articles. each item in this list is a dictionary\n",
    "    all_articles = response_data.get('articles', [])   \n",
    "    \n",
    "    # adds a semantic similarity score between the query and the article's description to the dictionary\n",
    "    get_similarities(all_articles, query)\n",
    "\n",
    "    # gets the N articles with the best semantic similarity scores\n",
    "    top_matches = get_n_matches(all_articles, n_search)\n",
    "\n",
    "    if blend:\n",
    "        top_matches = get_blend(all_articles, n_search)\n",
    "    \n",
    "    # rewrite the content of each article AND also return a string that's all of the articles content\n",
    "    top_content = extract_content(top_matches)\n",
    "    \n",
    "    # save this to a txt file for our own viewing\n",
    "    output_file = open(\"top_content.txt\", \"w\", encoding=\"utf-8\")\n",
    "    output_file.write(top_content)    \n",
    "    output_file.close()\n",
    "\n",
    "    # ask gemini to search the contents based on the query\n",
    "    prompt = \"Based soley on the articles provided, is it true that \"\n",
    "    prompt += query\n",
    "    gemini_response = ask_gemini(prompt, top_content, test_mode=False)    # set test mode to true to just call the function without the API. formatting purposes, avoiding too many API requests\n",
    "    print(gemini_response)\n",
    "\n",
    "    # return the response and the list of sources used\n",
    "    citation = get_citation(top_matches)\n",
    "    print(citation)\n",
    "    \n",
    "    # TODO: finish the list of sources for right, center, and left leaning sources. Limit is 5min a day. \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cee7976",
   "metadata": {},
   "source": [
    "https://newspaper.readthedocs.io/en/latest/\n",
    "- easy webscraping of news sources https://scrapeops.io/python-web-scraping-playbook/newspaper3k/\n",
    "\n",
    "\n",
    "News API gets you the most relevant articles, authors, description etc. \n",
    "BUT it cannot get you the actual content of each article but newspaper3k can. it can also give us the main keywords, although it's not reliable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13511acc",
   "metadata": {},
   "source": [
    "now that we have all the stories for a certain keyword in one string we have several options\n",
    " - use it as ground truth fact checker\n",
    "   - remember that this is based on a time range from when we set our API request to and what keyword\n",
    " - provide a summary of it all\n",
    "   - convinenet because we also a have a list of sources used\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977b4815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SUMMARIZATION\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "model_name = 't5-small'\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# basic summarization function\n",
    "def summarize(text):\n",
    "    inputs = tokenizer.encode(\"summarize: \" + text, return_tensors=\"pt\", max_length=5120, truncation=True)\n",
    "    summary_ids = model.generate(inputs, max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    return summary\n",
    "\n",
    "# basic summarizer, note its not very good.\n",
    "# misses a lot of key details and ignores many stories, even after increasing the max length 5 fold. \n",
    "# probably needs some fine tuning\n",
    "text = all_articles\n",
    "summary = summarize(text)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3aee94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine tune summarization https://huggingface.co/docs/transformers/en/tasks/summarization\n",
    "# use the provided descriptions as targets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
